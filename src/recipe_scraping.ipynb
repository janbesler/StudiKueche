{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from recipe_scrapers import scrape_html\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bon Appetit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting recipe URLs: 51 pages [04:12,  4.95s/ pages]\n",
      "Scraping recipes: 100%|██████████| 1192/1192 [50:25<00:00,  2.54s/ recipes] \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from recipe_scrapers import scrape_html\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_recipe_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    links = soup.find_all('a', \n",
    "                        class_=['BaseLink-eNWuiM', 'Link-ehXskl'],\n",
    "                        href=lambda x: x and '/recipe/' in x)\n",
    "    \n",
    "    recipe_urls = []\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            full_url = f\"https://www.bonappetit.com{href}\" if href.startswith('/') else href\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    return list(set(recipe_urls))\n",
    "\n",
    "def get_next_page_url(soup, base_url):\n",
    "    next_button = soup.find('a', \n",
    "                           class_=['BaseButton-bLlsy', 'ButtonWrapper-xCepQ'],\n",
    "                           string='Next')\n",
    "    if next_button and next_button.get('href'):\n",
    "        return urljoin(base_url, next_button.get('href'))\n",
    "    return None\n",
    "\n",
    "def clean_recipe_data(data):\n",
    "    keys_to_keep = [\n",
    "        'canonical_url', 'description', 'ingredient_groups', \n",
    "        'ingredients', 'instructions', 'instructions_list', \n",
    "        'keywords', 'ratings', 'ratings_count', 'site_name', 'title'\n",
    "    ]\n",
    "    return {k: v for k, v in data.items() if k in keys_to_keep}\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.bonappetit.com/simple-cooking/weeknight-meals\"\n",
    "    all_recipe_urls = []\n",
    "    current_url = base_url\n",
    "    failed_recipes = []\n",
    "    \n",
    "    # First progress bar for collecting URLs\n",
    "    pbar_urls = tqdm(desc=\"Collecting recipe URLs\", unit=\" pages\")\n",
    "    \n",
    "    while current_url:\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(current_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            page_recipes = get_recipe_links(current_url)\n",
    "            all_recipe_urls.extend(page_recipes)\n",
    "            \n",
    "            current_url = get_next_page_url(soup, base_url)\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            pbar_urls.update(1)\n",
    "            \n",
    "        except Exception:\n",
    "            break\n",
    "    \n",
    "    pbar_urls.close()\n",
    "    all_recipe_urls = list(set(all_recipe_urls))\n",
    "    \n",
    "    # Second progress bar for scraping recipes\n",
    "    recipes_dict = {}\n",
    "    pbar_scraping = tqdm(total=len(all_recipe_urls), desc=\"Scraping recipes\", unit=\" recipes\")\n",
    "    \n",
    "    for url in all_recipe_urls:\n",
    "        try:\n",
    "            html = urlopen(url).read().decode(\"utf-8\")\n",
    "            scraper = scrape_html(html, org_url=url)\n",
    "            recipe_data = scraper.to_json()\n",
    "            \n",
    "            # Clean data and store with URL key\n",
    "            recipe_key = url.split('/recipe/')[-1]\n",
    "            recipes_dict[recipe_key] = clean_recipe_data(recipe_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_recipes.append({\n",
    "                'url': url,\n",
    "                'name': url.split('/recipe/')[-1],\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        pbar_scraping.update(1)\n",
    "    \n",
    "    pbar_scraping.close()\n",
    "    \n",
    "    # Print summary of failed recipes\n",
    "    if failed_recipes:\n",
    "        print(f\"\\nFailed to scrape {len(failed_recipes)} recipes:\")\n",
    "        for failed in failed_recipes:\n",
    "            print(f\"- {failed['name']}: {failed['url']}\")\n",
    "    \n",
    "    return recipes_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recipes = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Convert dictionary to table with proper schema\n",
    "df = pd.DataFrame.from_dict(recipes, orient='index')\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'bonappetit_recipes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chefkoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting recipe URLs: 24 pages [01:26,  3.60s/ pages]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on page 24: 502 Server Error: Bad Gateway for url: https://www.chefkoch.de/rs/s24t49,50r4.5p30/Schnell-Einfach-Rezepte.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping recipes: 100%|██████████| 1051/1051 [1:06:48<00:00,  3.81s/ recipes]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully scraped 1043 recipes\n",
      "Saved recipes to chefkoch_recipes_20250120_171102.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from recipe_scrapers import scrape_html\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "\n",
    "def get_recipe_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.encoding = 'utf-8'\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', class_='ds-recipe-card__link')\n",
    "    \n",
    "    recipe_urls = []\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            clean_url = href.split('#')[0]\n",
    "            recipe_urls.append(clean_url)\n",
    "    \n",
    "    return list(set(recipe_urls))\n",
    "\n",
    "def clean_recipe_data(data):\n",
    "    keys_to_keep = [\n",
    "        'canonical_url', 'description', 'ingredient_groups',\n",
    "        'ingredients', 'instructions', 'instructions_list',\n",
    "        'keywords', 'ratings', 'ratings_count', 'site_name', 'title'\n",
    "    ]\n",
    "    return {k: v for k, v in data.items() if k in keys_to_keep}\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.chefkoch.de/rs/s{page}t49,50r4.5p30/Schnell-Einfach-Rezepte.html\"\n",
    "    urls_file = 'chefkoch_recipe_urls.json'\n",
    "    max_recipes = 2000\n",
    "    \n",
    "    # Try to load existing URLs\n",
    "    try:\n",
    "        with open(urls_file, 'r', encoding='utf-8') as f:\n",
    "            all_recipe_urls = json.load(f)\n",
    "        print(f\"Loaded {len(all_recipe_urls)} existing URLs\")\n",
    "    except FileNotFoundError:\n",
    "        all_recipe_urls = []\n",
    "    \n",
    "    # Only collect new URLs if we don't have enough\n",
    "    if len(all_recipe_urls) < max_recipes:\n",
    "        pbar_urls = tqdm(desc=\"Collecting recipe URLs\", unit=\" pages\")\n",
    "        page = 0\n",
    "        \n",
    "        while len(all_recipe_urls) < max_recipes:\n",
    "            try:\n",
    "                current_url = base_url.format(page=page)\n",
    "                page_recipes = get_recipe_links(current_url)\n",
    "                \n",
    "                if not page_recipes:\n",
    "                    break\n",
    "                    \n",
    "                all_recipe_urls.extend(page_recipes)\n",
    "                all_recipe_urls = list(set(all_recipe_urls))  # Remove duplicates\n",
    "                \n",
    "                # Save URLs after each page\n",
    "                with open(urls_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(all_recipe_urls, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "                pbar_urls.update(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page}: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        pbar_urls.close()\n",
    "    \n",
    "    # Limit to max_recipes\n",
    "    all_recipe_urls = list(set(all_recipe_urls))[:max_recipes]\n",
    "    \n",
    "    # Second progress bar for scraping recipes\n",
    "    recipes_dict = {}\n",
    "    failed_recipes = []\n",
    "    pbar_scraping = tqdm(total=len(all_recipe_urls), desc=\"Scraping recipes\", unit=\" recipes\")\n",
    "    \n",
    "    for url in all_recipe_urls:\n",
    "        try:\n",
    "            html = urlopen(url).read().decode(\"utf-8\")\n",
    "            scraper = scrape_html(html, org_url=url)\n",
    "            recipe_data = scraper.to_json()\n",
    "            \n",
    "            recipe_key = url.split('/')[-1].replace('.html', '')\n",
    "            recipes_dict[recipe_key] = clean_recipe_data(recipe_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_recipes.append({\n",
    "                'url': url,\n",
    "                'name': url.split('/')[-1],\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        pbar_scraping.update(1)\n",
    "    \n",
    "    pbar_scraping.close()\n",
    "    \n",
    "    # Print summary of failed recipes\n",
    "    if failed_recipes:\n",
    "        print(f\"\\nFailed to scrape {len(failed_recipes)} recipes:\")\n",
    "        for failed in failed_recipes:\n",
    "            print(f\"- {failed['name']}: {failed['url']}\")\n",
    "        \n",
    "        # Save failed recipes\n",
    "        with open('failed_recipes.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(failed_recipes, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nSuccessfully scraped {len(recipes_dict)} recipes\")\n",
    "    \n",
    "    # Save to parquet with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    df = pd.DataFrame.from_dict(recipes_dict, orient='index')\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    output_file = f'chefkoch_recipes_{timestamp}.parquet'\n",
    "    pq.write_table(table, output_file)\n",
    "    print(f\"Saved recipes to {output_file}\")\n",
    "    \n",
    "    return recipes_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recipes = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Feline86',\n",
       " 'canonical_url': 'https://www.chefkoch.de/rezepte/1748901284210089/Dicke-Obst-Pfannkuchen.html',\n",
       " 'category': 'Dessert',\n",
       " 'cook_time': 20,\n",
       " 'description': 'Dicke Obst-Pfannkuchen - wie auf der Kirmes. Über 1162 Bewertungen und für vorzüglich befunden. Mit ► Portionsrechner ► Kochbuch ► Video-Tipps!',\n",
       " 'host': 'chefkoch.de',\n",
       " 'image': 'https://img.chefkoch-cdn.de/rezepte/1748901284210089/bilder/1394501/crop-960x540/dicke-obst-pfannkuchen.jpg',\n",
       " 'ingredient_groups': [{'ingredients': ['250 g Mehl',\n",
       "    '75 g Zucker',\n",
       "    '3 m.-große Ei(er)',\n",
       "    '2 Pck. Vanillinzucker',\n",
       "    '200 ml Milch',\n",
       "    '½ TL Salz',\n",
       "    '2 große Äpfel oder 400 g Heidelbeeren oder 1 Glas Kirschen oder 400 g Johannisbeeren',\n",
       "    'Zucker zum Bestreuen',\n",
       "    'Butter , neutrales Öl oder Margarine'],\n",
       "   'purpose': None}],\n",
       " 'ingredients': ['250 g Mehl',\n",
       "  '75 g Zucker',\n",
       "  '3 m.-große Ei(er)',\n",
       "  '2 Pck. Vanillinzucker',\n",
       "  '200 ml Milch',\n",
       "  '½ TL Salz',\n",
       "  '2 große Äpfel oder 400 g Heidelbeeren oder 1 Glas Kirschen oder 400 g Johannisbeeren',\n",
       "  'Zucker zum Bestreuen',\n",
       "  'Butter , neutrales Öl oder Margarine'],\n",
       " 'instructions': 'Die Eigelbe vom Eiweiß trennen. Das Obst vorbereiten: z.B. Äpfel schälen, entkernen, achteln und in feine Scheiben schneiden oder die Beeren verlesen. \\r\\n\\r\\nDie Eigelbe mit Zucker, Vanillinzucker, Mehl, Milch und Salz zu einem glatten Teig verrühren. Die Eiweiße zu steifem Schnee schlagen. Zunächst das Obst unter den Teig rühren, dann den Eischnee vorsichtig unterheben.\\r\\n\\r\\nFett in einer Pfanne zergehen lassen und dicke Pfannkuchen darin bei schwacher bis mittlerer Hitze goldbraun backen. Die Pfannkuchen auf einen Teller legen und mit Zucker bestreuen. \\r\\n\\r\\nErgibt etwa 12 Pfannkuchen. \\r\\n\\r\\nTipp: Die Pfannkuchen nicht zu lange backen. Am besten schmecken sie, wenn sie innen noch schön weich sind. \\r\\n\\r\\nSie passen sehr gut als Beilage zu Rheinischer Bohnensuppe (siehe meine Rezepte) oder zu anderen süßen Gerichten wie Milchreis.',\n",
       " 'instructions_list': ['Die Eigelbe vom Eiweiß trennen. Das Obst vorbereiten: z.B. Äpfel schälen, entkernen, achteln und in feine Scheiben schneiden oder die Beeren verlesen. \\r',\n",
       "  '\\r',\n",
       "  'Die Eigelbe mit Zucker, Vanillinzucker, Mehl, Milch und Salz zu einem glatten Teig verrühren. Die Eiweiße zu steifem Schnee schlagen. Zunächst das Obst unter den Teig rühren, dann den Eischnee vorsichtig unterheben.\\r',\n",
       "  '\\r',\n",
       "  'Fett in einer Pfanne zergehen lassen und dicke Pfannkuchen darin bei schwacher bis mittlerer Hitze goldbraun backen. Die Pfannkuchen auf einen Teller legen und mit Zucker bestreuen. \\r',\n",
       "  '\\r',\n",
       "  'Ergibt etwa 12 Pfannkuchen. \\r',\n",
       "  '\\r',\n",
       "  'Tipp: Die Pfannkuchen nicht zu lange backen. Am besten schmecken sie, wenn sie innen noch schön weich sind. \\r',\n",
       "  '\\r',\n",
       "  'Sie passen sehr gut als Beilage zu Rheinischer Bohnensuppe (siehe meine Rezepte) oder zu anderen süßen Gerichten wie Milchreis.'],\n",
       " 'keywords': ['Backen',\n",
       "  'Europa',\n",
       "  'Vegetarisch',\n",
       "  'Schnell',\n",
       "  'einfach',\n",
       "  'Frühstück',\n",
       "  'Deutschland',\n",
       "  'Süßspeise',\n",
       "  'Dessert',\n",
       "  'Kinder',\n",
       "  'Mehlspeisen'],\n",
       " 'language': 'de',\n",
       " 'nutrients': {'servingSize': '1',\n",
       "  'calories': '493 kcal',\n",
       "  'proteinContent': '12,99g',\n",
       "  'fatContent': '11,29g',\n",
       "  'carbohydrateContent': '84,86g'},\n",
       " 'prep_time': 20,\n",
       " 'ratings': 4.72,\n",
       " 'ratings_count': 1162,\n",
       " 'site_name': 'Chefkoch',\n",
       " 'title': 'Dicke Obst-Pfannkuchen',\n",
       " 'total_time': 40,\n",
       " 'yields': '4 servings'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.to_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
